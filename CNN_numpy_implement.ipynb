{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_numpy_implement.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stephen2run/colab/blob/master/CNN_numpy_implement.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "miraOmcGsnPO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy import signal\n",
        "\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "def d_tanh(x):\n",
        "    return 1 - np.tanh(x) ** 2\n",
        "\n",
        "def log(x):\n",
        "    return 1/(1 + np.exp(-1*x))\n",
        "def d_log(x):\n",
        "    return log(x) * ( 1 - log(x) )\n",
        "\n",
        "np.random.seed(598765)\n",
        "\n",
        "x1 = np.array([[0,0,0],[0,0,0],[0,0,0]])\n",
        "x2 = np.array([[1,1,1],[0,0,0],[0,0,0]])\n",
        "x3 = np.array([[0,0,0],[1,1,1],[1,1,1]])\n",
        "x4 = np.array([ [1,1,1],[1,1,1],[1,1,1]])\n",
        "X = [x1,x2,x3,x4]\n",
        "Y = np.array([\n",
        "    [0.53],\n",
        "    [0.77],\n",
        "    [0.88],\n",
        "    [1.1]\n",
        "])\n",
        "\n",
        "# 0. Declare Weights\n",
        "w1 = np.random.randn(2,2) * 4 \n",
        "w2 = np.random.randn(4,1) * 4\n",
        "\n",
        "# 1. Declare hyper Parameters\n",
        "num_epoch = 1000\n",
        "learning_rate = 0.7\n",
        "\n",
        "cost_before_train = 0\n",
        "cost_after_train = 0\n",
        "final_out,start_out =np.array([[]]),np.array([[]])\n",
        "\n",
        "# ---- Cost before training ------\n",
        "for i in range(len(X)):\n",
        "    \n",
        "    layer_1 = signal.convolve2d(X[i],w1,'valid')\n",
        "    layer_1_act = tanh(layer_1)\n",
        "\n",
        "    layer_1_act_vec = np.expand_dims(np.reshape(layer_1_act,-1),axis=0)\n",
        "    layer_2 = layer_1_act_vec.dot(w2)\n",
        "    layer_2_act = log(layer_2)   \n",
        "    cost = np.square(layer_2_act- Y[i]).sum() * 0.5\n",
        "    cost_before_train = cost_before_train + cost\n",
        "    start_out = np.append(start_out,layer_2_act)\n",
        "    \n",
        "# ----- TRAINING -------\n",
        "for iter in range(num_epoch):\n",
        "    \n",
        "    for i in range(len(X)):\n",
        "    \n",
        "        layer_1 = signal.convolve2d(X[i],w1,'valid')\n",
        "        layer_1_act = tanh(layer_1)\n",
        "\n",
        "        layer_1_act_vec = np.expand_dims(np.reshape(layer_1_act,-1),axis=0)\n",
        "        layer_2 = layer_1_act_vec.dot(w2)\n",
        "        layer_2_act = log(layer_2)\n",
        "\n",
        "        cost = np.square(layer_2_act- Y[i]).sum() * 0.5\n",
        "        #print(\"Current iter : \",iter , \" Current train: \",i, \" Current cost: \",cost,end=\"\\r\")\n",
        "\n",
        "        grad_2_part_1 = layer_2_act- Y[i]\n",
        "        grad_2_part_2 = d_log(layer_2)\n",
        "        grad_2_part_3 = layer_1_act_vec\n",
        "        grad_2 =   grad_2_part_3.T.dot(grad_2_part_1*grad_2_part_2)      \n",
        "\n",
        "        grad_1_part_1 = (grad_2_part_1*grad_2_part_2).dot(w2.T)\n",
        "        grad_1_part_2 = d_tanh(layer_1)\n",
        "        grad_1_part_3 = X[i]\n",
        "\n",
        "        grad_1_part_1_reshape = np.reshape(grad_1_part_1,(2,2))\n",
        "        grad_1_temp_1 = grad_1_part_1_reshape * grad_1_part_2\n",
        "        grad_1 = np.rot90(\n",
        "          signal.convolve2d(grad_1_part_3, np.rot90(grad_1_temp_1, 2),'valid'),\n",
        "          2)\n",
        "\n",
        "        w2 = w2 - grad_2 * learning_rate\n",
        "        w1 = w1 - grad_1 * learning_rate\n",
        "        \n",
        "# ---- Cost after training ------\n",
        "for i in range(len(X)):\n",
        "    \n",
        "    layer_1 = signal.convolve2d(X[i],w1,'valid')\n",
        "    layer_1_act = tanh(layer_1)\n",
        "\n",
        "    layer_1_act_vec = np.expand_dims(np.reshape(layer_1_act,-1),axis=0)\n",
        "    layer_2 = layer_1_act_vec.dot(w2)\n",
        "    layer_2_act = log(layer_2)   \n",
        "    cost = np.square(layer_2_act- Y[i]).sum() * 0.5\n",
        "    cost_after_train = cost_after_train + cost\n",
        "    final_out = np.append(final_out,layer_2_act)\n",
        "\n",
        "    \n",
        "# ----- Print Results ---\n",
        "print(\"\\nW1 :\",w1, \"\\n\\nw2 :\", w2)\n",
        "print(\"----------------\")\n",
        "print(\"Cost before Training: \",cost_before_train)\n",
        "print(\"Cost after Training: \",cost_after_train)\n",
        "print(\"----------------\")\n",
        "print(\"Start Out put : \", start_out)\n",
        "print(\"Final Out put : \", final_out)\n",
        "print(\"Ground Truth  : \", Y.T)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# -- end code --"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}